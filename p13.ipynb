{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”µ 1. MNIST Digit Detection using TensorFlow (Low-Level API, FIXED)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist as tf_mnist\n",
        "\n",
        "# ğŸ”¹ Data Preparation\n",
        "(x_train_tf, y_train_tf), (x_test_tf, y_test_tf) = tf_mnist.load_data()\n",
        "x_train_tf, x_test_tf = x_train_tf / 255.0, x_test_tf / 255.0\n",
        "\n",
        "train_ds_tf = tf.data.Dataset.from_tensor_slices((x_train_tf, y_train_tf)).shuffle(10000).batch(32)\n",
        "test_ds_tf = tf.data.Dataset.from_tensor_slices((x_test_tf, y_test_tf)).batch(32)\n",
        "\n",
        "# ğŸ”¹ Fixed Model Definition using tf.keras.Model\n",
        "class TFModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(10)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense1(x)\n",
        "        return self.dense2(x)\n",
        "\n",
        "model_tf = TFModel()\n",
        "loss_fn_tf = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer_tf = tf.keras.optimizers.Adam()\n",
        "\n",
        "# ğŸ”¹ Training\n",
        "@tf.function\n",
        "def train_step_tf(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model_tf(images)\n",
        "        loss = loss_fn_tf(labels, logits)\n",
        "    grads = tape.gradient(loss, model_tf.trainable_variables)\n",
        "    optimizer_tf.apply_gradients(zip(grads, model_tf.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "for epoch in range(10):\n",
        "    for images, labels in train_ds_tf:\n",
        "        loss = train_step_tf(images, labels)\n",
        "    print(f\"ğŸ”µ TF Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "# ğŸ”¹ Evaluation\n",
        "correct_tf = 0\n",
        "total_tf = 0\n",
        "for images, labels in test_ds_tf:\n",
        "    logits = model_tf(images)\n",
        "    predictions = tf.argmax(logits, axis=1)\n",
        "    correct_tf += tf.reduce_sum(tf.cast(predictions == tf.cast(labels, tf.int64), tf.float32))\n",
        "    total_tf += images.shape[0]\n",
        "\n",
        "print(f\"ğŸ”µ TF Test Accuracy: {correct_tf / total_tf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LLjq9xOC4QH",
        "outputId": "67aa34de-8660-4ea8-b532-0dd6a39a6b42"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”µ TF Epoch 1, Loss: 0.1545\n",
            "ğŸ”µ TF Epoch 2, Loss: 0.0219\n",
            "ğŸ”µ TF Epoch 3, Loss: 0.0147\n",
            "ğŸ”µ TF Epoch 4, Loss: 0.0118\n",
            "ğŸ”µ TF Epoch 5, Loss: 0.0335\n",
            "ğŸ”µ TF Epoch 6, Loss: 0.0468\n",
            "ğŸ”µ TF Epoch 7, Loss: 0.0140\n",
            "ğŸ”µ TF Epoch 8, Loss: 0.0201\n",
            "ğŸ”µ TF Epoch 9, Loss: 0.0071\n",
            "ğŸ”µ TF Epoch 10, Loss: 0.0245\n",
            "ğŸ”µ TF Test Accuracy: 0.9783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”µ 3. MNIST Digit Detection using Keras (TensorFlow Backend)\n",
        "from tensorflow.keras.datasets import mnist as keras_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# ğŸ”¹ Data Preparation\n",
        "(x_train_kr, y_train_kr), (x_test_kr, y_test_kr) = keras_mnist.load_data()\n",
        "x_train_kr, x_test_kr = x_train_kr / 255.0, x_test_kr / 255.0\n",
        "y_train_kr = to_categorical(y_train_kr)\n",
        "y_test_kr = to_categorical(y_test_kr)\n",
        "\n",
        "# ğŸ”¹ Model Definition\n",
        "model_kr = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# ğŸ”¹ Compilation\n",
        "model_kr.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# ğŸ”¹ Training\n",
        "model_kr.fit(x_train_kr, y_train_kr, batch_size=32, epochs=5, validation_split=0.2)\n",
        "\n",
        "# ğŸ”¹ Evaluation\n",
        "loss_kr, acc_kr = model_kr.evaluate(x_test_kr, y_test_kr)\n",
        "print(f\"ğŸ”µ Keras Test Loss: {loss_kr:.4f}\")\n",
        "print(f\"ğŸ”µ Keras Test Accuracy: {acc_kr:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAZOE-atC70V",
        "outputId": "97c457b9-bd21-4d54-99af-b204a9e532e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8685 - loss: 0.4718 - val_accuracy: 0.9498 - val_loss: 0.1684\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9593 - loss: 0.1387 - val_accuracy: 0.9643 - val_loss: 0.1200\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.0905 - val_accuracy: 0.9684 - val_loss: 0.1049\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9799 - loss: 0.0660 - val_accuracy: 0.9712 - val_loss: 0.0949\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9855 - loss: 0.0512 - val_accuracy: 0.9732 - val_loss: 0.0891\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9705 - loss: 0.0944\n",
            "ğŸ”µ Keras Test Loss: 0.0833\n",
            "ğŸ”µ Keras Test Accuracy: 0.9736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”µ 2. MNIST Digit Detection using PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ğŸ”¹ Data Preparation\n",
        "transform_pt = transforms.Compose([transforms.ToTensor()])\n",
        "train_data_pt = datasets.MNIST(root='./data', train=True, download=True, transform=transform_pt)\n",
        "test_data_pt = datasets.MNIST(root='./data', train=False, download=True, transform=transform_pt)\n",
        "train_loader_pt = DataLoader(train_data_pt, batch_size=32, shuffle=True)\n",
        "test_loader_pt = DataLoader(test_data_pt, batch_size=32, shuffle=False)\n",
        "\n",
        "# ğŸ”¹ Model Definition\n",
        "class NetPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetPT, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model_pt = NetPT()\n",
        "criterion_pt = nn.CrossEntropyLoss()\n",
        "optimizer_pt = optim.Adam(model_pt.parameters(), lr=0.001)\n",
        "\n",
        "# ğŸ”¹ Training\n",
        "for epoch in range(5):\n",
        "    for images, labels in train_loader_pt:\n",
        "        outputs = model_pt(images)\n",
        "        loss = criterion_pt(outputs, labels)\n",
        "        optimizer_pt.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_pt.step()\n",
        "    print(f\"ğŸ”µ PT Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# ğŸ”¹ Evaluation\n",
        "correct_pt = 0\n",
        "total_pt = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader_pt:\n",
        "        outputs = model_pt(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_pt += labels.size(0)\n",
        "        correct_pt += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"ğŸ”µ PT Test Accuracy: {100 * correct_pt / total_pt:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwle6KUQDFUr",
        "outputId": "226c8834-270d-49fb-caf3-475fc15b2d20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 17.6MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 478kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 4.44MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 6.01MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”µ PT Epoch 1, Loss: 0.2350\n",
            "ğŸ”µ PT Epoch 2, Loss: 0.0984\n",
            "ğŸ”µ PT Epoch 3, Loss: 0.0433\n",
            "ğŸ”µ PT Epoch 4, Loss: 0.0140\n",
            "ğŸ”µ PT Epoch 5, Loss: 0.0363\n",
            "ğŸ”µ PT Test Accuracy: 97.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ErT6xS7SDG78"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}